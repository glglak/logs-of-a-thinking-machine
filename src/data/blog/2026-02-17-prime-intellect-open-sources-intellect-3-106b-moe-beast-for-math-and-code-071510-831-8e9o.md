---
title: "Prime Intellect Open-Sources INTELLECT-3: 106B MoE Beast for Math and Code"
pubDatetime: 2026-02-17T07:15:10.831Z
description: "106B params, tops math/code benches, and fully open-sourced training stack—your next open model for agentic dev tools is here."
heroImage: "https://primeintellect.ai/intellect3-promo.jpg"
ogImage: "https://primeintellect.ai/intellect3-promo.jpg"
content_pillar: "software"
tags: ["open-source","moe","reasoning","codegen","rl","AI","digest"]
---

![Prime Intellect Open-Sources INTELLECT-3: 106B MoE Beast for Math and Code](https://primeintellect.ai/intellect3-promo.jpg)

> 106B params, tops math/code benches, and fully open-sourced training stack—your next open model for agentic dev tools is here.

**Open-source fans, rejoice: a 106B MoE model just crushed benchmarks via massive RL, and they gave away the whole recipe.**

Prime Intellect released INTELLECT-3 today in February 2026 news—a 106B parameter Mixture-of-Experts model excelling in math, code, and reasoning. They open-sourced everything: weights, training stack, and environments for true reproducibility.[1]

This empowers devs to fine-tune for custom agents, RAG pipelines, or math-heavy apps without black-box limits. RL-trained for verifiable performance, it fits the 2025 reasoning wave (GRPO, inference scaling).[1]

Beats prior opens like OLMo 3 (32B top open) on key metrics, challenging closed giants like o1 while handing the community frontier tools. No more 'benchmaxxing' excuses—full stack means real innovation.[1]

Download from Prime Intellect, spin up on your cluster, and experiment with RL environments. What's the first killer app you'll build?

**Source:** [Prime Intellect via Dentro.de](https://dentro.de/ai/news/)
